{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76660295",
   "metadata": {},
   "source": [
    "# Minimize the output error for structured pruning\n",
    "\n",
    "## Method\n",
    "\n",
    "### Output error\n",
    "\n",
    "考虑一个全连接层权重为 $W \\in R^{co \\times ci}$ ，输入激活为 $X \\in R^{N \\times ci}$，记稀疏权重为 $W_S \\in R^{co \\times ci}$，则剪枝前后的输出误差平方和为：\n",
    "\n",
    "$$\\text{error} = \\sum{(W_S X^T - W X^T)^2}$$\n",
    "\n",
    "优化目标为\n",
    "\n",
    "$$\\min_{W_S} {\\text{error}}$$\n",
    "\n",
    "考虑结构化剪枝，记 $W$ 的第 $i$ 列为 $W_{i}$，$X$ 的第 $i$ 列为 $X_{i}$，若将 $W_{i}$ 剪枝，则\n",
    "\n",
    "$$\\text{error}= \\sum{(W_{i} X_{i}^T)^2} = X_{i}^T X_{i} W_{i}^T W_{i} $$\n",
    "\n",
    "若将 $X_{i}$ 和$X_{j}$剪枝，则\n",
    "\n",
    "$$ \\text{error} = \\sum{(W_{i} X_{i}^T + W_{j} X_{j}^T)^2} = X_{i}^T X_{i} W_{i}^T W_{i} + X_{j}^T X_{j} W_{j}^T W_{j} + X_{i}^T X_{j} W_{i}^T W_{j} + X_{j}^T X_{i} W_{j}^T W_{i}$$\n",
    "\n",
    "若将 $X_{i}$，$X_{j}$ 和 $X_{k}$ 剪枝，则...\n",
    "\n",
    "记 $S_W = W^T W$，$S_X = X^T X$，$S = S_W * S_X$，剪枝通道集合为$P$，则剪枝前后的输出误差平方和为：\n",
    "\n",
    "$$\\text{error} = \\sum_{i \\in P}\\sum_{j \\in P}{S_{i,j}}$$\n",
    "\n",
    "即矩阵$S$主子式之和。\n",
    "\n",
    "### Greedy strategy\n",
    "\n",
    "记稀疏度为$sp$，需要剪枝的通道数量为$cs = \\lfloor ci * sp \\rfloor)$，最小化error即需要从$S$中选出一个和最小的$cs$阶主子式。\n",
    "\n",
    "通过递归可以搜索所有可能的$cs$阶主子式，找到其中和最小的主子式，即该问题的解。\n",
    "\n",
    "然而，这并不现实，这样的主子式共有$C_{ci}^{cs}$个，对于大模型而言，以llama2为例，最小的7B模型hidden_size=4096。\n",
    "\n",
    "一个可行的策略是，选取矩阵$S$对角线上的元素$\\text{score}$作为重要性度量，其代表修剪某一输入通道时的error，其和wanda非常相似，不妨称为channel_wanda。\n",
    "\n",
    "进一步，考虑能否通过更新$\\text{score}$，使得每一次修剪的分数与前面所有修剪的分数和为矩阵$S$主子式之和，一个简单的方法如下：\n",
    "\n",
    "$$\\text{score} = \\text{score} + 2S_i$$\n",
    "\n",
    "$$\\text{score}[i] = \\infty $$\n",
    "\n",
    "当某一次修剪第$i$列时，将矩阵$S$的第$i$行和第$i$列加到$\\text{score}$中，注意到$S$为实对称矩阵。将该列的分数置为无穷，标记为选中。\n",
    "\n",
    "显然，若第一次修剪为第5列，将矩阵$S$的第$5$行和第$5$列加到$\\text{score}$中后，第二次修剪无论选择哪一列，其对应的分数与第一次修剪的分数之和，均是\n",
    "该列与第5列构成的一个二阶主子式之和。\n",
    "\n",
    "同时，每次修剪时，我们贪心地选取分数最小的列即可，即\n",
    "\n",
    "$$\\argmin_{i}{\\text{score}}$$。\n",
    "\n",
    "### Details\n",
    "\n",
    "剪枝的对象是所有Decoder中self_attn的o_proj以及mlp中的down_proj，修剪它们的输入通道后，再修剪v_proj（对应o_proj）和gate_proj、up_proj\n",
    "（对应down_proj）对应的输出通道，注意不修剪q_proj和k_proj对应的输出通道，因为它们的输出将按注意力头分组后作点积注意力，其输出并不与o_proj逐通道\n",
    "对应。\n",
    "\n",
    "llm-pruner剪枝的对象是部分Decoder中的注意力头和mlp的隐层神经元，通常不修剪前面几层Decoder以及最后一个Decoder。\n",
    "\n",
    "## Experiment\n",
    "\n",
    "time 为空是以前的测试结果，time 非空是torch.manual_seed(993)下的测试结果。\n",
    "\n",
    "wikitext2 困惑度指标越小越好，其它为精确度指标，越大越好。\n",
    "\n",
    "dense 6738415616\n",
    "\n",
    "(method sparsity remain_parameters)\n",
    "\n",
    "sp = 0.2 / llm 0.25 5372383232 / channel_wanda 0.23 5364509674 / fix_channel_wanda 0.23 5365165094 \n",
    "\n",
    "method(dataset/sample num/seq_len)\n",
    "\n",
    "| llama2-7B                     | time | wikitext2 |winogrande|hellaswag|boolq|piqa|openbookqa|arc_easy|arc_challenge| avg     |\n",
    "|-------------------------------|------|-----------|---|---|---|---|---|---|---|---------|\n",
    "| dense                         | -    | 9.98      |0.6709|0.5668|0.7009|0.7845|0.3180|0.6932|0.3985| 0.5904  |\n",
    "| llm-pruner(bookcorpus/10/64)  | -    | 16.19     |0.5825|0.4926|0.5896|0.7606|0.2640|0.6553|0.3925| 0.5338  |\n",
    "| llm-pruner(c4/10/128)         | 3.3  | 29.99     |0.6038|0.4899|0.6422|0.7459|0.2900|0.6031|0.3404| 0.5307  |\n",
    "| llm-pruner(bookcorpus/100/64) | -    | 15.38     |0.6022|0.4803|0.5572|0.7557|0.2640|0.6170|0.3379| 0.5163  |\n",
    "| channel_wanda(c4/32/128)      | 2.6  | 19.61     |0.6433|0.5142|0.6394|0.7612|0.2640|0.6477|0.3695| 0.5484  |\n",
    "| channel_wanda(c4/64/128)      | 4.1  | 17.27     |0.6417|0.5220|0.6343|0.7650|0.2780|0.6317|0.3669| 0.5485  |\n",
    "| fix_channel_wanda(c4/32/128)  | 41.8 | 15.50     |0.6377|0.5057|0.6474|0.7552|0.2700|0.6204|0.3234| 0.5371  |\n",
    "| fix_channel_wanda(c4/64/128)  | 40.4 | 16.94     |0.6622|0.5232|0.6768|0.7535|0.2720|0.6414|0.3422| 0.5530  |\n",
    "| fix_channel_wanda(c4/96/128)  | 16.5 | 16.91     |0.6646|0.5272|0.6713|0.7693|0.2700|0.6591|0.3712| 0.5618  |\n",
    "| fix_channel_wanda(c4/108/128) | 17.9 | 17.4      |0.6504|0.5220|0.6465|0.7612|0.2640|0.6524|0.3584| 0.5507  |\n",
    "\n",
    "fix_channel_wanda(c4/96/128)、fix_channel_wanda(c4/108/128)优化了代码。显存不够，使用fp16。\n",
    "\n",
    "sp=0.4 / llm 0.5 4006350848 / channel_wanda 0.48 4013945624 / fix_channel_wanda 0.48 4014601043\n",
    "\n",
    "| llama2-7B                    | time | wikitext2 |winogrande|hellaswag|boolq|piqa|openbookqa|arc_easy|arc_challenge|avg|\n",
    "|------------------------------|------|-----------|---|---|---|---|---|---|---|---|\n",
    "| dense                        | -    | 9.98      |0.6709|0.5668|0.7009|0.7845|0.3180|0.6932|0.3985|0.5904|\n",
    "| llm-pruner(bookcorpus/10/64) | 3.3  | 53.90     |0.5264|0.3467|0.6086|0.6469|0.1820|0.3893|0.2406|0.4200|\n",
    "| llm-pruner(c4/10/128)        | 2.0  | 55.32     |0.5146|0.3619|0.6162|0.6915|0.2060|0.4061|0.2363|0.4332|\n",
    "| channel_wanda(c4/10/128)     | 3.3  | 66.87     |0.5406|0.3581|0.6089|0.6513|0.1820|0.4297|0.2594|0.4328|\n",
    "| channel_wanda(c4/32/128)     | 6.2  | 63.12     |0.5414|0.3933|0.4755|0.6877|0.2100|0.4499|0.2662|0.432|\n",
    "| fix_channel_wanda(c4/10/128) | 92.0 | 48.62     |0.5272|0.3534|0.4165|0.6371|0.1700|0.4381|0.2457|0.3982|\n",
    "| fix_channel_wanda(c4/32/128) | 98.2 | 46.41     |0.5572|0.3838|0.5997|0.6763|0.1900|0.4327|0.2594|0.4427|\n",
    "| fix_channel_wanda(c4/64/128) | 29.2 | 52.67     |0.5588|0.4029|0.6171|0.6752|0.2060|0.4865|0.2747|0.4601|\n",
    "| fix_channel_wanda(c4/96/128) | 29.5 | 48.71     |0.5706|0.4146|0.6012|0.6861|0.2400|0.5164|0.2884|0.4739|\n",
    "\n",
    "fix_channel_wanda(c4/64/128) fix_channel_wanda(c4/96/128) 优化了代码。显存不够，使用fp16。\n",
    "\n",
    "## Discussion\n",
    "\n",
    "### llm-pruner\n",
    "llm-pruner 不能剪模型的前面几层Decoder和后面几层Decoder，否则性能会迅速下降，以llama2为例，其修剪4-30层（序号0-31）。\n",
    "\n",
    "channel_wanda 和 fix_channel_wanda 则可以。\n",
    "\n",
    "原因可能在于它们修剪对象中self_attn部分的不同。llm-pruner在这部分中修剪注意力头，每修剪一个注意力头，就修剪其对应的o_proj的若干输入通道，以及\n",
    "q_proj、k_proj、v_proj的若干输出通道，实际上，注意力头将整个运算分成了多组缩放点积注意力运算，llama2-7B中，一个注意力头对应128个输入通道（o）\n",
    "和128个输出通道（qkv）。channel_wanda 和 fix_channel_wanda 没有注意力头的约束，直接修剪o_proj的输入通道，因为不能与q_proj、k_proj的输出\n",
    "通道逐个对应，因而没有裁剪。\n",
    "\n",
    "llm-pruner 提到采用channel策略（修剪norm及其对应的q_proj、k_proj、v_proj的输入通道），将不可避免的修剪模型前面几层Decoder和后面几层\n",
    "Decoder（残差连接连通了整个模型的所有Decoder的norm），在作者的实验中，前四层和最后一层Decoder对剪枝的敏感度很高，修剪后困惑度大幅提升，因此不宜\n",
    "进行修剪，在 channel_wanda 和 fix_channel_wanda 看来，或许是修剪q_proj、k_proj的原因？\n",
    "\n",
    "此外，llm-pruner 修剪神经元或注意力头并不完全依赖于某一层，例如修剪mlp隐层神经元，用到gate_proj、up_proj、down_proj与神经元相连的权重的分数\n",
    "和，channel_wanda 和 fix_channel_wanda 仅考虑 down_proj 计算得到的分数。\n",
    "\n",
    "**fix_channel_wanda 或可借助 llm-pruner 的框架**：之前仅做了bookcorpus 10 个样本的实验，表现上比llm-pruner差（ppl=15.16，avg=0.5125），\n",
    "推测增加样本量可能获得提升。\n",
    "\n",
    "### wanda\n",
    "\n",
    "容易发现，当$co=1$时，矩阵$S$的对角线元素即为wanda分数的平方形式，当$co>1$时，矩阵$S$的对角线元素即为wanda分数的平方形式沿着输出通道维度的和，故\n",
    "将直接使用矩阵$S$的对角线元素作为分数的方法称为channel_wanda。\n",
    "\n",
    "此外，矩阵$S_X = X^T X$为Hessian矩阵，这些分数形式上与wanda和sparsegpt或许存在某些关联。\n",
    "\n",
    "### other baseline\n",
    "\n",
    "#### ZipLM: Inference-Aware Structured Pruning of Language Models\n",
    "\n",
    "结构化裁剪版本的sparseGPT。\n",
    "\n",
    "创新点，每裁掉一个通道后更新一次黑森矩阵的逆。不是很懂。\n",
    "\n",
    "Algorithm 1 The ZipLM pruning algorithm. Given inverse Hessian $\\mathbf{H} ^{- 1}= ( 2\\mathbf{X} \\mathbf{X} ^{\\top }+ \\lambda \\mathbf{I} ) ^{- 1}$,we remove exactly $k$ structures from the corresponding weight matrix W.\n",
    "\n",
    "$\\mathbf{R}\\leftarrow$set of all possible structures\n",
    "\n",
    "for $k$ times do\n",
    "\n",
    "$\\mathbf{S}\\leftarrow$argmin$_\\mathbf{S}\\sum_{i=0}^{d_\\mathrm{row}}\\mathbf{W}_{i,\\mathbf{M_S}}\\cdot((\\mathbf{H}^{-1})_{\\mathbf{M_S},\\mathbf{M_S}})^{-1}\\cdot\\mathbf{W}_{i,\\mathbf{M_S}}^{\\top}$\n",
    "\n",
    "$\\delta_{S}\\leftarrow-\\mathbf{W}_{:,\\mathbf{M_{S}}}\\cdot((\\mathbf{H}^{-1})_{\\mathbf{M_{S}},\\mathbf{M_{S}}})^{-1}\\cdot(\\mathbf{H}^{-1})_{\\mathbf{M_{S}},:}$\n",
    "\n",
    "$\\begin{array}{l}\\mathbf{W}\\leftarrow\\mathbf{W}+\\boldsymbol{\\delta}\\boldsymbol{s}\\\\\\mathbf{H}^{-1}\\leftarrow\\mathbf{H}^{-1}-\\mathbf{H}_{:,\\mathbf{M}\\mathbf{S}}^{-1}\\cdot((\\mathbf{H}^{-1})_{\\mathbf{M}\\mathbf{S}},\\mathbf{M}_\\mathbf{S})^{-1}\\cdot\\mathbf{H}_{\\mathbf{M}\\mathbf{S},:}^{-1}\\end{array}$\n",
    "\n",
    "$\\mathbf{R}\\leftarrow\\mathbf{R}-\\{\\mathbf{S}\\}$\n",
    "\n",
    "end for\n",
    "\n",
    "$\\mathbf{W}\\leftarrow\\mathbf{W}\\odot\\mathbf{M}_{\\mathbf{R}}$\n",
    "\n",
    "#### THE NEED FOR SPEED PRUNING TRANSFORMERS WITH ONE RECIPE\n",
    "\n",
    "OPTIN 框架\n",
    "\n",
    "特点是one-shot，随机抽取一个小批量数据对模型剪枝，剪枝后不需要re-train。\n",
    "\n",
    "逐次遮蔽第i层的第j个神经元以计算其重要性，重要性由两个部分组成。首先是中间特征的差异，中间特征的表示为reshape为BTxD后与reshape的转置相乘得到BTxBT，层的中间特征损失定义为遮蔽前后表示差值的二范数，并从第i+1层开始累加。第二是遮蔽前后输出概率值的变化，计算两者的相对熵，类似于知识蒸馏中的软标签。\n",
    "\n",
    "上述损失一般用来指导训练，作者将其作为神经元重要性，两者用超参数平衡，0.1 or 0.01。\n",
    "\n",
    "主要剪枝对象是注意力头和前馈神经网络的隐层神经元。\n",
    "\n",
    "对于注意力头，huggingface的transformers库中**class transformers.ViTModel**\n",
    "\n",
    "的前向提供head_mask参数。\n",
    "\n",
    "**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`。\n",
    "\n",
    "对于隐层神经元，作者的做法是使用hook将经过ffn第一个linear层后待激活的特征在指定神经元处屏蔽。\n",
    "\n",
    "### how many data samples are needed\n",
    "\n",
    "相比于channel_wanda，fix_channel_wanda的困惑度更低，但性能却不一定更好。\n",
    "\n",
    "这似乎取决于标定数据集样本的数量，当数量较少时，fix_channel_wanda 表现上不如 channel_wanda，这或许是因为 fix_channel_wanda 更加注重于最\n",
    "小化输出的误差，即它对标定数据集“过拟合”了，这使得它在标定数据集上具有更小的损失，但泛化性能却不见得更好。\n",
    "\n",
    "而 channel_wanda 的性能对标定数据集大小不太敏感，数据集增大时可能进一步降低困惑度，但性能几乎没有提升。这与wanda对数据集的鲁棒非常相似。\n",
    "\n",
    "llm-pruner 的性能随数据集增大似乎会降低，作者在原文中只采用了bookcorpus的10个样本，且没有论述标定数据集大小的影响。\n",
    "\n",
    "当数据集增大时，三者的困惑度都降低。数据集较大时，fix_channel_wanda困惑度回升，似乎缓解了“过拟合”。\n",
    "\n",
    "能否增加数据多样性来减少所需标定数据？采用相同域的数据应该会更好。\n",
    "\n",
    "# Minimize the output error for Unstructured pruning\n",
    "\n",
    "## Method\n",
    "\n",
    "### Unstructured version\n",
    "\n",
    "注意到前文结构化剪枝中$S_X$与输出通道无关，因此可以逐通道的裁剪，实现无结构化剪枝。\n",
    "\n",
    "前文剪枝考虑的是整个层输出误差的平方和$\\text{error}$，现在考虑逐通道输出误差的平方和$\\text{error}_{i}$，事实上，整个层输出误差的平\n",
    "方和$\\text{error}$（经过非结构化剪枝的）等于逐通道输出误差的平方和$\\text{error}_{i}$的累加。本质上依旧是最小化$\\text{error}$，只是\n",
    "逐通道的去最小化。\n",
    "\n",
    "如果直接计算每个通道的矩阵$S$，计算量无疑极大。一个可行的策略是，只在每次更新时计算矩阵$S$对应的行，而不是提取计算好。\n",
    "\n",
    "只考虑一个通道，记稀疏度为$sp$，需要剪枝的数量为$cs = \\lfloor ci * sp \\rfloor)$，score初始化\n",
    "\n",
    "$$\\text{score} = W^2 diag(S_X)$$\n",
    "\n",
    "同样地，每次修剪时，我们贪心地选取分数最小的索引即可，即\n",
    "\n",
    "$$\\argmin_{i}{\\text{score}}$$。\n",
    "\n",
    "$$\\text{score}[i] = \\infty $$\n",
    "\n",
    "更新分数值\n",
    "\n",
    "$$\\text{score} = \\text{score} + 2 W W_i * S_Xi$$\n",
    "\n",
    "为加快计算，实际修剪时所有通道一起修剪，对应的索引为一个长度为$c_o$向量，对应各通道修剪的索引。\n",
    "\n",
    "```python\n",
    "@torch.no_grad()\n",
    "def fix_prune(weight, inputs, s,\n",
    "              lamda=1., fp16=False): \n",
    "    co, ci = weight.shape\n",
    "    if fp16:\n",
    "        inputs = inputs.reshape((-1, ci)).type(torch.float32)\n",
    "        o_weight = weight\n",
    "        weight = o_weight.type(torch.float32)\n",
    "    else:\n",
    "        inputs = inputs.reshape((-1, ci))\n",
    "    sb = inputs.T @ inputs\n",
    "\n",
    "    score = (weight ** 2) * torch.diag(sb)\n",
    "    rows = torch.arange(co)\n",
    "\n",
    "    prune_num = int(ci * s)\n",
    "    prune_idx = []\n",
    "    while len(prune_idx) < prune_num:\n",
    "        idx = torch.kthvalue(score, k=2, dim=1)[1]\n",
    "        prune_idx.append(idx.reshape(co, 1))\n",
    "\n",
    "        if lamda > 0.:\n",
    "            change = weight * weight[rows, idx].reshape(co, 1) * sb[idx]\n",
    "            score += 2 * lamda * change\n",
    "            del change\n",
    "\n",
    "        score[rows, idx] = torch.inf\n",
    "    prune_idx = torch.cat(prune_idx, dim=1)\n",
    "    if fp16:\n",
    "        o_weight.scatter_(dim=1, index=prune_idx, value=0)\n",
    "        true_s = torch.sum(o_weight == 0).item() / weight.numel()\n",
    "        del weight, inputs\n",
    "    else:\n",
    "        weight.scatter_(dim=1, index=prune_idx, value=0)\n",
    "        true_s = torch.sum(weight == 0).item() / weight.numel()\n",
    "    assert abs(true_s - s) < 0.001\n",
    "    del sb, score, prune_idx\n",
    "```\n",
    "\n",
    "## Experiment\n",
    "\n",
    "torch.manual_seed(993)\n",
    "\n",
    "method(dataset/sample num/seq_len)\n",
    "\n",
    "### sample\n",
    "\n",
    "s=0.7\n",
    "\n",
    "与前面类似，随着样本数量的上升，精确率逐步上升，ppl逐渐下降，然而样本数量足够多，几乎不再增长，最终水平与wanda类似，仅在ppl强于wanda。\n",
    "\n",
    "| llama2-7B                 | time   | wikitext2 | winogrande |hellaswag|boolq|piqa|openbookqa|arc_easy|arc_challenge| avg     |\n",
    "|---------------------------|--------|-----------|------------|---|---|---|---|---|---|---------|\n",
    "| dense                     | -      | 9.98      | 0.6709     |0.5668|0.7009|0.7845|0.3180|0.6932|0.3985| 0.5904  |\n",
    "| wanda(c4/10/128)          | -      | 102.44    | 0.5288     |0.2976|0.6223|0.6028|0.1440|0.3758|0.1894|0.3943|\n",
    "| fix_wanda(c4/64/128)      | -      | 132.95    | 0.5288     |0.2884|0.6116|0.5767|0.1300|0.3047|0.1852|0.3750|\n",
    "| fix_wanda(c4/120/128)     | -      | 101.56    | 0.5351     |0.2953|0.6217|0.5843|0.1360|0.3194|0.1903|0.3831|\n",
    "| fix_wanda(c4/160/128)     | -      | 104.00    | 0.5170     |0.3010|0.6217|0.5996|0.1460|0.3413|0.2014|0.3897|\n",
    "| fix_wanda(c4/256/128)     | -      | 89.84     | 0.5288     |0.3065|0.6217|0.6050|0.1500|0.3590|0.2125|0.3976|\n",
    "| fix_wanda(c4/320/128)     | -      | 86.34     | 0.5375     |0.3067|0.6217|0.6012|0.1400|0.3409|0.1988|0.3924|\n",
    "\n",
    "### lamda\n",
    "\n",
    "s=0.7\n",
    "\n",
    "修改lamda进行观察，没看出什么。\n",
    "\n",
    "| llama2-7B                     | time   | wikitext2 | winogrande |hellaswag|boolq|piqa|openbookqa|arc_easy|arc_challenge| avg     |\n",
    "|-------------------------------|--------|-----------|------------|---|---|---|---|---|---|---------|\n",
    "| dense                         | -      | 9.98      | 0.6709     |0.5668|0.7009|0.7845|0.3180|0.6932|0.3985| 0.5904  |\n",
    "| wanda(c4/10/128)              | -      | 102.44    | 0.5288     |0.2976|0.6223|0.6028|0.1440|0.3758|0.1894|0.3943|\n",
    "| fix_wanda_0.1(c4/48/128)      | 1701.1 | 85.92     | 0.5185     |0.3049|0.6217|0.5968|0.1440|0.3489|0.1980|0.3904|\n",
    "| fix_wanda_0.2(c4/48/128)      | 1698.8 | 80.40     | 0.5296     |0.3024|0.6220|0.5941|0.1460|0.3384|0.1962|0.3898|\n",
    "| fix_wanda_0.25(c4/45/128)     | 1699.4 | 87.74     | 0.5422     |0.3045|0.6217|0.6001|0.1580|0.3577|0.2022|0.3980|\n",
    "| fix_wanda_0.3(c4/48/128)      | 1699.1 | 79.28     | 0.5399     |0.3103|0.6214|0.5985|0.1520|0.3649|0.1971|0.3977|\n",
    "| fix_wanda_0.3(c4/80/128)      | 1702.8 | 79.84     | 0.5422     |0.3083|0.6217|0.5925|0.1500|0.3653|0.1971|0.3967|\n",
    "| fix_wanda_0.35(c4/48/128)     | 1700.8 | 88.97     | 0.5328     |0.3014|0.6217|0.5936|0.1380|0.3342|0.1911|0.3875|\n",
    "| fix_wanda_0.4(c4/48/128)      | 1703.7 | 91.05     | 0.5122     |0.3042|0.6217|0.6034|0.1440|0.3489|0.1971|0.3902|\n",
    "| fix_wanda_0.5(c4/48/128)      | 1716.6 | 93.22     | 0.5359     |0.3018|0.6217|0.5952|0.1380|0.3476|0.2022|0.3917|\n",
    "| fix_wanda_0.5(c4/128/128)     | -      | 82.48     | 0.5375     |0.3058|0.6217|0.6050|0.1440|0.3569|0.1980|0.3955|\n",
    "| fix_wanda_0.6(c4/48/128)      | 1700.3 | 103.49    | 0.5296     |0.2939|0.6196|0.5919|0.1500|0.3270|0.1852|0.3853|\n",
    "| fix_wanda_0.7(c4/48/128)      | 1698.4 | 106.17    | 0.5162     |0.2912|0.6223|0.5729|0.1380|0.3093|0.1937|0.3776|\n",
    "| fix_wanda_0.7(c4/128/128)     | -      | 86.51     | 0.5391     |0.3085|0.6217|0.6061|0.1560|0.3472|0.1988|0.3967|\n",
    "| fix_wanda_0.7(c4/256/128)     | -      | 77.88     | 0.5367     |0.3109|0.6217|0.6104|0.1440|0.3561|0.2048|0.3978|\n",
    "| fix_wanda_0.8(c4/120/128)     | -      | 96.03     | 0.5399     |0.3038|0.6214|0.5947|0.1400|0.3434|0.1962|0.3913|\n",
    "| fix_wanda_0.9(c4/120/128)     | -      | 105.54    | 0.5406     |0.2976|0.6217|0.5974|0.1520|0.3380|0.2039|0.3930|\n",
    "\n",
    "### prune target\n",
    "\n",
    "s=0.7，对不同层及其组合进行修剪，似乎目标是q_proj、k_proj和v_proj时效果较好（其它层使用wanda修剪）。\n",
    "\n",
    "| llama2-7B                     | time   | wikitext2 | winogrande |hellaswag|boolq|piqa|openbookqa|arc_easy|arc_challenge| avg   |\n",
    "|-------------------------------|--------|-----------|------------|---|---|---|---|---|---|-------|\n",
    "| dense                         | -      | 9.98      | 0.6709     |0.5668|0.7009|0.7845|0.3180|0.6932|0.3985| 0.5904|\n",
    "| wanda(c4/128/128)             | 13.0   | 92.71     | 0.5209     |0.2965|0.6214|0.6012|0.1360|0.3535|0.1852|0.3878|\n",
    "| wanda(c4/256/128)             | 25.0   | 92.57     | 0.5335     |0.2966|0.6217|0.6028|0.1380|0.3691|0.1886|0.3929|\n",
    "| wanda(c4/512/128)             | 16.4   | 95.38     | 0.5359     |0.2954|0.6217|0.6017|0.1380|0.3695|0.1877|0.3928|\n",
    "| fix_wanda_o(c4/128/128)       | -      | 104.32    | 0.5264     |0.2931|0.6208|0.5860|0.1440|0.3401|0.1971|0.3867|\n",
    "| fix_wanda_q(c4/128/128)       | -      | 91.45     | 0.5201     |0.3035|0.6217|0.6072|0.1480|0.3742|0.1937|0.3954|\n",
    "| fix_wanda_q(c4/256/128)       | -      | 82.28     | 0.5233     |0.3045|0.6217|0.6066|0.1580|0.3847|0.1928|0.3988|\n",
    "| fix_wanda_k(c4/128/128)       | -      | 86.13     | 0.5264     |0.3061|0.6217|0.6083|0.1520|0.3859|0.2039|0.4006|\n",
    "| fix_wanda_k(c4/256/128)       | -      | 83.31     | 0.5257     |0.3037|0.6217|0.6181|0.1520|0.3830|0.1997|0.4005|\n",
    "| fix_wanda_v(c4/128/128)       | -      | 91.72     | 0.5493     |0.3013|0.6217|0.6088|0.1460|0.3523|0.2048|0.3977|\n",
    "| fix_wanda_v(c4/256/128)       | -      | 87.36     | 0.5217     |0.3054|0.6217|0.6126|0.1420|0.3868|0.2065|0.3995|\n",
    "| fix_wanda_d(c4/128/128)       | -      | 107.09    | 0.5201     |0.2931|0.6217|0.5871|0.1360|0.3464|0.1903|0.3849|\n",
    "| fix_wanda_up(c4/128/128)      | -      | 118.96    | 0.5399     |0.2933|0.6220|0.5892|0.1440|0.3405|0.1980|0.3895|\n",
    "| fix_wanda_gate(c4/128/128)    | -      | 115.58    | 0.5343     |0.2924|0.6217|0.6001|0.1500|0.3316|0.1869|0.3881|\n",
    "| fix_wanda_qk(c4/256/128)      | -      | 76.85     | 0.5367     |0.3137|0.6217|0.6148|0.1540|0.4003|0.2031|0.4063|\n",
    "| fix_wanda_kv(c4/256/128)      | -      | 81.85     | 0.5288     |0.3123|0.6217|0.6197|0.1520|0.3902|0.2065|0.4044|\n",
    "| fix_wanda_qv(c4/256/128)      | -      | 78.73     | 0.5288     |0.3142|0.6217|0.6219|0.1500|0.3986|0.2125|0.4068|\n",
    "| fix_wanda_qkv(c4/128/128)     | -      | 76.36     | 0.5272     |0.3135|0.6220|0.6208|0.1480|0.3796|0.2159|0.4038|\n",
    "| fix_wanda_qkv(c4/256/128)     | 548.7  | 74.92     | 0.5304     |0.3185|0.6217|0.6192|0.1520|0.4066|0.2065|0.4078|\n",
    "| fix_wanda_qkv(c4/512/128)     | 383.8  | 77.03     | 0.5249     |0.3190|0.6220|0.6235|0.1400|0.4032|0.2142|0.4066|\n",
    "| fix_wanda_0.7_qkv(c4/256/128) | -      | 74.17     | 0.5367     |0.3182|0.6232|0.6159|0.1460|0.4091|0.2056|0.4078|\n",
    "\n",
    "### qkv experiment\n",
    "\n",
    "对q_proj、k_proj和v_proj使用fix_wanda，其它层使用wanda修剪，用 fix_wanda_qkv 指代。\n",
    "\n",
    "50% 稀疏度下，表现与wanda基本无差异。60% 稀疏度下，表现与wanda似乎也没有太大差异。\n",
    "\n",
    "70% 稀疏度下，三个模型上表现平均值超出wanda1.5、2、2.5个百分点。80% 稀疏度下，出现意想不到的情况，几个模型几个方法的结果趋同，\n",
    "可能需要更大的模型在该稀疏度下测试。\n",
    "\n",
    "Llama3在高稀疏度下表现更差，或与GQA减小k、v大小有关（llama2 4096*4096，llama3 1024*4096）。\n",
    "\n",
    "另外，在llama3上，高稀疏度下，似乎对所有层使用fix_wanda，表现上可以超过wanda，但依旧弱于fix_wanda_qkv。\n",
    "\n",
    "\n",
    "s=0.5\n",
    "\n",
    "| llama2-7B                 | time  | wikitext2 | winogrande |hellaswag|boolq|piqa|openbookqa|arc_easy|arc_challenge| avg     |\n",
    "|---------------------------|-------|-----------|------------|---|---|---|---|---|---|---------|\n",
    "| dense                     | -     | 9.98      | 0.6709     |0.5668|0.7009|0.7845|0.3180|0.6932|0.3985| 0.5904  |\n",
    "| wanda(c4/10/128)          | -     | 13.43     | 0.6646     |0.5160|0.7208|0.7606|0.2720|0.6439|0.3549|0.5618|\n",
    "| wanda(c4/256/128)         | -     | 13.32     | 0.6622     |0.5190|0.7220|0.7677|0.2800|0.6629|0.3532|0.5667|\n",
    "| fix_wanda(c4/128/128)     | -     | 13.99     | 0.6598     |0.5049|0.7131|0.7524|0.2740|0.6397|0.3464|0.5557|\n",
    "| fix_wanda(c4/256/128)     | -     | 13.78     | 0.6654     |0.5081|0.7052|0.7595|0.2800|0.6620|0.3464|0.5609|\n",
    "| fix_wanda(c4/340/128)     | -     | 13.56     | 0.6630     |0.5078|0.6963|0.7590|0.2600|0.6532|0.3439|0.5547|\n",
    "| fix_wanda_qkv(c4/256/128) | 404.7 | 13.09     | 0.6543     |0.5229|0.7211|0.7601|0.2740|0.6620|0.3609|0.5650|\n",
    "\n",
    "\n",
    "| llama3-8B-instruct-tune   | time   | wikitext2 | winogrande  |hellaswag|boolq|piqa|openbookqa|arc_easy|arc_challenge| avg     |\n",
    "|---------------------------|--------|-----------|-------------|---|---|---|---|---|---|---------|\n",
    "| dense                     | -      | -         | 0.7253      |0.5899|0.8370|0.7954|0.3440|0.8304|0.5512|0.6676|\n",
    "| wanda(book/10/128)        | -      | -         | 0.6819      |0.5041|0.7963|0.7552|0.2960|0.7555|0.4266|0.6022|\n",
    "| wanda(c4/128/128)         | 15.8   | 20.93     | 0.6827      |0.5105|0.8049|0.7655|0.2840|0.7504|0.4317|0.6042|\n",
    "| wanda(c4/256/128)         | -      | 20.86     | 0.6898      |0.5127|0.8015|0.7671|0.2880|0.7504|0.4326|0.6060|\n",
    "| fix_wanda(c4/128/128)     | -      | 22.60     | 0.6867      |0.4996|0.7994|0.7633|0.2720|0.7475|0.4121|0.5972|\n",
    "| fix_wanda(c4/256/128)     | 1612.3 | 21.30     | 0.6953      |0.5029|0.7801|0.7579|0.2780|0.7601|0.4428|0.6024|\n",
    "| fix_wanda(c4/300/128)     | -      | 21.43     | 0.6993      |0.5068|0.7838|0.7671|0.2840|0.7572|0.4266|0.6035|\n",
    "| fix_wanda_qkv(c4/256/128) | 232.9  | 20.74     | 0.6772      |0.5140|0.8043|0.7661|0.2960|0.7605|0.4420|0.6085|\n",
    "\n",
    "\n",
    "| llama3-8B                 | time | wikitext2 | winogrande |hellaswag|boolq|piqa|openbookqa|arc_easy|arc_challenge| avg     |\n",
    "|---------------------------|------|-----------|------------|---|---|---|---|---|---|---------|\n",
    "| dense                     | -    | 12.86     | 0.7261     |0.6019|0.8159|0.7965|0.3480|0.8009|0.5034|0.6561|\n",
    "| wanda(c4/256/128)         | 30.5 | 18.99     | 0.6977     |0.5148|0.7550|0.7655|0.2840|0.7142|0.4019|0.5904|\n",
    "| fix_wanda_qkv(c4/256/128) |      | 19.07     | 0.6961     |0.5232|0.7440|0.7628|0.2840|0.7180|0.4036|0.5902|\n",
    "\n",
    "\n",
    "s=0.6\n",
    "\n",
    "| llama2-7B                 | time  | wikitext2 | winogrande |hellaswag|boolq|piqa|openbookqa|arc_easy|arc_challenge| avg     |\n",
    "|---------------------------|-------|-----------|------------|---|---|---|---|---|---|---------|\n",
    "| dense                     | -     | 9.98      | 0.6709     |0.5668|0.7009|0.7845|0.3180|0.6932|0.3985| 0.5904  |\n",
    "| wanda(c4/256/128)         | -     | 20.10     | 0.6251     |0.4400|0.6630|0.7220|0.2380|0.5955|0.2986|0.5117|\n",
    "| fix_wanda_qkv(c4/256/128) | -     | 19.91     | 0.6204     |0.4449|0.6532|0.7203|0.2220|0.5972|0.3046|0.5089|\n",
    "\n",
    "| llama3-8B-instruct-tune   | time  | wikitext2 | winogrande |hellaswag|boolq|piqa|openbookqa|arc_easy|arc_challenge| avg    |\n",
    "|---------------------------|-------|-----------|------------|---|---|---|---|---|---|--------|\n",
    "| dense                     | -     | -         | 0.7253     |0.5899|0.8370|0.7954|0.3440|0.8304|0.5512| 0.6676 |\n",
    "| wanda(c4/128/128)         | -     | 42.60     | 0.6022     |0.4044|0.7217|0.7133|0.2160|0.6637|0.3413| 0.5232 |\n",
    "| wanda(c4/256/128)         |\n",
    "| fix_wanda(c4/128/128)     | 15.8  | 50.64     | 0.5991     |0.3874|0.6911|0.6980|0.1960|0.6452|0.3003|0.5024|\n",
    "| fix_wanda_qkv(c4/256/128) | 271.7 | 40.89     | 0.6022     |0.4151|0.7291|0.7220|0.2300|0.6768|0.3515|0.5323|\n",
    "\n",
    "| llama3-8B             | time  | wikitext2 | winogrande |hellaswag|boolq|piqa|openbookqa|arc_easy|arc_challenge| avg     |\n",
    "|-----------------------|-------|-----------|------------|---|---|---|---|---|---|---------|\n",
    "| dense                 | -     | 12.86     | 0.7261     |0.6019|0.8159|0.7965|0.3480|0.8009|0.5034|0.6561|\n",
    "| wanda(c4/256/128)     | 30.4  | 35.30     | 0.6188     |0.4083|0.6835|0.7095|0.2320|0.6208|0.3029|0.5108|\n",
    "| fix_wanda(c4/256/128) | 279.0 | 34.77     | 0.6188     |0.4166|0.6945|0.7040|0.2260|0.6351|0.3072|0.5146|\n",
    "\n",
    "\n",
    "s=0.7\n",
    "\n",
    "| llama2-7B                  | time  | wikitext2 | winogrande |hellaswag|boolq|piqa|openbookqa|arc_easy|arc_challenge| avg   |\n",
    "|----------------------------|-------|-----------|------------|---|---|---|---|---|---|-------|\n",
    "| dense                      | -     | 9.98      | 0.6709     |0.5668|0.7009|0.7845|0.3180|0.6932|0.3985| 0.5904|\n",
    "| wanda(c4/128/128)          | 13.0  | 92.71     | 0.5209     |0.2965|0.6214|0.6012|0.1360|0.3535|0.1852|0.3878|\n",
    "| wanda(c4/256/128)          | 25.0  | 92.57     | 0.5335     |0.2966|0.6217|0.6028|0.1380|0.3691|0.1886|0.3929|\n",
    "| wanda(c4/512/128)          | 16.4  | 95.38     | 0.5359     |0.2954|0.6217|0.6017|0.1380|0.3695|0.1877|0.3928|\n",
    "| fix_wanda_qkv(c4/256/128)  | 548.7 | 74.92     | 0.5304     |0.3185|0.6217|0.6192|0.1520|0.4066|0.2065|0.4078|\n",
    "\n",
    "\n",
    "| llama3-8B-instruct-tune   | time  | wikitext2 | winogrande |hellaswag|boolq|piqa|openbookqa|arc_easy|arc_challenge| avg     |\n",
    "|---------------------------|-------|-----------|------------|---|---|---|---|---|---|---------|\n",
    "| dense                     | -     | -         | 0.7253     |0.5899|0.8370|0.7954|0.3440|0.8304|0.5512|0.6676|\n",
    "| wanda(book/10/128)        | -     | -         | 0.4759     |0.2763|0.4006|0.5626|0.1120|0.3182|0.1809|0.3323|\n",
    "| wanda(c4/128/128)         | 15.8  | 220.46    | 0.5178     |0.2877|0.4511|0.5952|0.1300|0.3885|0.1792|0.3642|\n",
    "| wanda(c4/256/128)         | -     | 217.85    | 0.5170     |0.2888|0.4758|0.6055|0.1380|0.3910|0.1775|0.3705|\n",
    "| fix_wanda(c4/128/128)     | -     | 211.51    | 0.5004     |0.2899|0.6223|0.5898|0.1220|0.3611|0.1766|0.3803|\n",
    "| fix_wanda(c4/256/128)     | -     | 199.60    | 0.5043     |0.2942|0.6153|0.6045|0.1240|0.3817|0.1741|0.3854|\n",
    "| fix_wanda_qkv(c4/256/128) | 320.3 | 186.97    | 0.5225     |0.2963|0.5624|0.6197|0.1400|0.4158|0.1817|0.3912|\n",
    "\n",
    "\n",
    "| llama3-8B                 | time  | wikitext2 | winogrande |hellaswag|boolq|piqa|openbookqa|arc_easy|arc_challenge| avg     |\n",
    "|---------------------------|-------|-----------|------------|---|---|---|---|---|---|---------|\n",
    "| dense                     | -     | 12.86     | 0.7261     |0.6019|0.8159|0.7965|0.3480|0.8009|0.5034|0.6561|\n",
    "| wanda(c4/256/128)         | 30.5  | 178.80    | 0.5146     |0.2805|0.4997|0.6007|0.1300|0.3973|0.1843|0.3724|\n",
    "| fix_wanda_qkv(c4/256/128) | 312.3 | 160.96    | 0.5391     |0.2981|0.6174|0.6023|0.1300|0.4146|0.1928|0.3991|\n",
    "\n",
    "\n",
    "s=0.8\n",
    "\n",
    "| llama2-7B               | time  | wikitext2 | winogrande |hellaswag|boolq|piqa|openbookqa|arc_easy|arc_challenge| avg   |\n",
    "|-------------------------|-------|-----------|------------|---|---|---|---|---|---|-------|\n",
    "| dense                   | -     | 9.98      | 0.6709     |0.5668|0.7009|0.7845|0.3180|0.6932|0.3985| 0.5904|\n",
    "| wanda(c4/256/128)       | 25.0  | 1408.6    | 0.5028     |0.2609|0.3783|0.5239|0.1400|0.2534|0.2056|0.3235|\n",
    "| fix_wanda_o(c4/256/128) | 623.8 | 689.70    | 0.5075     |0.2606|0.3783|0.5332|0.1320|0.2597|0.2090|0.3257|\n",
    "\n",
    "| llama3-8B-instruct-tune   | time   | wikitext2 | winogrande |hellaswag|boolq|piqa|openbookqa|arc_easy|arc_challenge| avg     |\n",
    "|---------------------------|--------|-----------|------------|---|---|---|---|---|---|---------|\n",
    "| dense                     | -      | -         | 0.7253     |0.5899|0.8370|0.7954|0.3440|0.8304|0.5512|0.6676|\n",
    "| wanda(c4/256/128)         | -      |\n",
    "| fix_wanda_qkv(c4/256/128) | 361.9  | 1225.70   | 0.5091     |0.2644|0.3783|0.5370|0.1260|0.2757|0.1894|0.3257|\n",
    "\n",
    "\n",
    "| llama3-8B                 | time | wikitext2 | winogrande |hellaswag|boolq|piqa|openbookqa|arc_easy|arc_challenge| avg     |\n",
    "|---------------------------|------|-----------|------------|---|---|---|---|---|---|---------|\n",
    "| dense                     | -    | 12.86     | 0.7261     |0.6019|0.8159|0.7965|0.3480|0.8009|0.5034|0.6561|\n",
    "| wanda(c4/256/128)         | -    |\n",
    "| fix_wanda_qkv(c4/256/128) | -    |\n",
    "\n",
    "### same field data\n",
    "\n",
    "前面提到fix_wanda仅对qkv进行修剪时，能够取得一定的效果，这与理论不对吻合，理论上对所有层进行修剪，效果应该更好。\n",
    "\n",
    "前文结构化剪枝提到该方法会对数据“过拟合”，即在标定数据上error很小，但对测试数据并不一定，fix_channel_wanda通过增加样本数量来对抗“过拟合”。\n",
    "\n",
    "然而，非结构化剪枝的fix_wanda“过拟合”能力是惊人的，它在每一层的error约为wanda的5%，对标定数据的过度拟合导致它泛化性能的下降。\n",
    "\n",
    "注意到c4数据集与测试数据集并不在同一个域，如果使用同域数据集会如何呢？\n",
    "\n",
    "从 wikitext的训练集中采样，fix_wanda_qkv 在 wikitext2 测试集上的表现远胜于 wanda。\n",
    "\n",
    "从每一个测试数据集对应的训练集中采样，每个数据集采样70个样本，len=64，组成混合的标定数据集，70%稀疏度下，可以看到fix_wanda_qkv(+1.99%)\n",
    "在测试集的表现高于 wanda(+0.86%)， 令人惊异的是， fix_wanda(+3.97%) 获得了极大的提升，同时击败了 wanda 和 fix_wanda_qkv。\n",
    "\n",
    "从 alpaca-cleaned 训练集采样256个样本，len=64，该数据集用于微调模型（剪枝后），格式为“instruction-input-output”。不妨认为该数据集在域上\n",
    "相比于C4更接近测试集，但不如mix接近，70%稀疏度下，从表现上看，wanda基本没有提升，fix_wanda_qkv 提升 0.99%，fix_wanda 提升1.53%，\n",
    "仍低于fix_wanda_qkv。\n",
    "\n",
    "这似乎说明，wanda 对于数据域不太敏感，fix_wanda 生效的关键是同域数据，如果我们有着明确的下游任务，从其训练集中采样标定数据，\n",
    "那么fix_wanda修剪后的模型在测试集上必然有更好的表现。\n",
    "\n",
    "值得注意的是，boolq似乎对使用何种数据修剪不敏感，70%稀疏度下，维持在62%。这可能是因为 boolq 的问题较为简单，与常识有关，且回答只有“是”或“否”。\n",
    "\n",
    "s=0.7\n",
    "\n",
    "| llama2-7B                   | time   | wikitext2 | winogrande |hellaswag|boolq|piqa|openbookqa|arc_easy|arc_challenge|avg|\n",
    "|-----------------------------|--------|-----------|------------|---|---|---|---|---|---|-----|\n",
    "| dense                       | -      | 9.98      | 0.6709     |0.5668|0.7009|0.7845|0.3180|0.6932|0.3985| 0.5904|\n",
    "| wanda(c4/256/128)           | 25.0   | 92.57     | 0.5335     |0.2966|0.6217|0.6028|0.1380|0.3691|0.1886|0.3929|\n",
    "| fix_wanda_qkv(c4/256/128)   | 548.7  | 74.92     | 0.5304     |0.3185|0.6217|0.6192|0.1520|0.4066|0.2065|0.4078|\n",
    "| fix_wanda(c4/256/128)       | -      | 89.84     | 0.5288     |0.3065|0.6217|0.6050|0.1500|0.3590|0.2125|0.3976|\n",
    "| wanda(wiki/256/128)         | -      | 73.57     | 0.5312     |0.2873|0.6205|0.5745|0.1320|0.3455|0.2022|0.3847|\n",
    "| fix_wanda_qkv(wiki/256/128) | -      | 48.16     | 0.5233     |0.2982|0.6229|0.5930|0.1460|0.3716|0.2090|0.3948|\n",
    "| fix_wanda(wiki/256/128)     |\n",
    "| wanda(mix/80*7/64)          | -      | 86.05     | 0.5162     |0.3018|0.6217|0.6072|0.1520|0.4112|0.2005|0.4015|\n",
    "| fix_wanda_qkv(mix/80*7/64)  | -      | 68.53     | 0.5193     |0.3263|0.6211|0.6502|0.1660|0.4844|0.2270|0.4277|\n",
    "| fix_wanda(mix/80*7/64)      | _      | 85.31     | 0.5422     |0.3163|0.6214|0.6556|0.1840|0.4979|0.2372|0.4363|\n",
    "| wanda(tune/256/128)         | 25.0   | 97.38     | 0.5217     |0.2931|0.6196|0.5941|0.1440|0.3859|0.2125|0.3958|\n",
    "| fix_wanda_qkv(tune/256/128) | 551.1  | 83.19     | 0.5343     |0.3152|0.6223|0.6240|0.1580|0.4487|0.2218|0.4177|\n",
    "| fix_wanda(tune/256/128)     | 2724.9 | 107.84    | 0.5375     |0.3045|0.6211|0.6066|0.1660|0.4280|0.2270|0.4129|\n",
    "\n",
    "s=0.6\n",
    "\n",
    "| llama2-7B                   | time  | wikitext2 | winogrande |hellaswag|boolq|piqa|openbookqa|arc_easy|arc_challenge| avg     |\n",
    "|-----------------------------|-------|-----------|------------|---|---|---|---|---|---|---------|\n",
    "| dense                       | -     | 9.98      | 0.6709     |0.5668|0.7009|0.7845|0.3180|0.6932|0.3985| 0.5904  |\n",
    "| wanda(c4/256/128)           | -     | 20.10     | 0.6251     |0.4400|0.6630|0.7220|0.2380|0.5955|0.2986|0.5117|\n",
    "| fix_wanda_qkv(c4/256/128)   | -     | 19.91     | 0.6204     |0.4449|0.6532|0.7203|0.2220|0.5972|0.3046|0.5089|\n",
    "| wanda(mix/80*7/64)          | -     |\n",
    "| fix_wanda_qkv(mix/80*7/64)  | -     |\n",
    "| fix_wanda(mix/80*7/64)      | _     |\n",
    "\n",
    "\n",
    "### channel corruption\n",
    "\n",
    "ria 的思想，避免 channel corruption，fix_wanda 确实更少一些，但这似乎并不是主要因素。\n",
    "\n",
    "前面的c4实验中，尽管 fix_wanda channel corruption 更少，但表现并没有更好。\n",
    "\n",
    "s = 0.7\n",
    "\n",
    "| method                     | corruption channels | percent |\n",
    "|----------------------------|---------------------|---------|\n",
    "| wanda(mix/80*7/64)         | 2520                | 0.0022  |\n",
    "| fix_wanda_qkv(mix/80*7/64) | 1875                | 0.0016  |\n",
    "| fix_wanda(mix/80*7/64)     | 468                 | 0.0004  |\n",
    "\n",
    "在以下代码中，将lamda置为0，wide=True，修剪得到的结果相比于wanda，channel corruption更少，但表现稍差。\n",
    "\n",
    "```python\n",
    "@torch.no_grad()\n",
    "def fix_prune(weight, inputs, s, wide=False,\n",
    "              lamda=1., fp16=False): \n",
    "    co, ci = weight.shape\n",
    "    if fp16:\n",
    "        inputs = inputs.reshape((-1, ci)).type(torch.float32)\n",
    "        o_weight = weight\n",
    "        weight = o_weight.type(torch.float32)\n",
    "    else:\n",
    "        inputs = inputs.reshape((-1, ci))\n",
    "    sb = inputs.T @ inputs\n",
    "\n",
    "    score = (weight ** 2) * torch.diag(sb)\n",
    "    rows = torch.arange(co)\n",
    "    if wide:\n",
    "        cnt = co * torch.ones((ci,), dtype=weight.dtype, device=weight.device)\n",
    "        sub = -torch.ones((co,), dtype=weight.dtype, device=weight.device)\n",
    "\n",
    "    prune_num = int(ci * s)\n",
    "    prune_idx = []\n",
    "    while len(prune_idx) < prune_num:\n",
    "        if wide:\n",
    "            idx = torch.argmin(score / cnt.clamp(min=1), dim=1)\n",
    "            cnt.scatter_add_(0, idx, sub)\n",
    "        else:\n",
    "            # idx = torch.argmin(score, dim=1)\n",
    "            idx = torch.kthvalue(score, k=2, dim=1)[1]\n",
    "        prune_idx.append(idx.reshape(co, 1))\n",
    "\n",
    "        if lamda > 0.:\n",
    "            change = weight * weight[rows, idx].reshape(co, 1) * sb[idx]\n",
    "            score += 2 * lamda * change\n",
    "            del change\n",
    "\n",
    "        score[rows, idx] = torch.inf\n",
    "        # score *= 1.001\n",
    "    prune_idx = torch.cat(prune_idx, dim=1)\n",
    "    if fp16:\n",
    "        o_weight.scatter_(dim=1, index=prune_idx, value=0)\n",
    "        true_s = torch.sum(o_weight == 0).item() / weight.numel()\n",
    "        del weight, inputs\n",
    "    else:\n",
    "        weight.scatter_(dim=1, index=prune_idx, value=0)\n",
    "        true_s = torch.sum(weight == 0).item() / weight.numel()\n",
    "    assert abs(true_s - s) < 0.001\n",
    "    del sb, score, prune_idx\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
